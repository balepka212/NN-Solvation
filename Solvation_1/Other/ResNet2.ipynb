{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "verbose = False\n",
    "verboseprint = print if verbose else lambda *a, **k: None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "resnet for 1-d signal data, pytorch version\n",
    "\n",
    "Shenda Hong, Oct 2019\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.data[index], dtype=torch.float), torch.tensor(self.label[index], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class MyConv1dPadSame(nn.Module):\n",
    "    \"\"\"\n",
    "    extend nn.Conv1d to support SAME padding\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups=1):\n",
    "        super(MyConv1dPadSame, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            groups=self.groups)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        net = x\n",
    "\n",
    "        # compute pad shape\n",
    "        in_dim = net.shape[-1]\n",
    "        out_dim = (in_dim + self.stride - 1) // self.stride\n",
    "        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\n",
    "        pad_left = p // 2\n",
    "        pad_right = p - pad_left\n",
    "        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\n",
    "\n",
    "        net = self.conv(net)\n",
    "\n",
    "        return net\n",
    "\n",
    "class MyMaxPool1dPadSame(nn.Module):\n",
    "    \"\"\"\n",
    "    extend nn.MaxPool1d to support SAME padding\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(MyMaxPool1dPadSame, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = 1\n",
    "        self.max_pool = torch.nn.MaxPool1d(kernel_size=self.kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        net = x\n",
    "\n",
    "        # compute pad shape\n",
    "        in_dim = net.shape[-1]\n",
    "        out_dim = (in_dim + self.stride - 1) // self.stride\n",
    "        p = max(0, (out_dim - 1) * self.stride + self.kernel_size - in_dim)\n",
    "        pad_left = p // 2\n",
    "        pad_right = p - pad_left\n",
    "        net = F.pad(net, (pad_left, pad_right), \"constant\", 0)\n",
    "\n",
    "        net = self.max_pool(net)\n",
    "\n",
    "        return net\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet Basic Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, downsample, use_bn, use_do, is_first_block=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.downsample = downsample\n",
    "        if self.downsample:\n",
    "            self.stride = stride\n",
    "        else:\n",
    "            self.stride = 1\n",
    "        self.is_first_block = is_first_block\n",
    "        self.use_bn = use_bn\n",
    "        self.use_do = use_do\n",
    "\n",
    "        # the first conv\n",
    "        self.bn1 = nn.BatchNorm1d(in_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.do1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = MyConv1dPadSame(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=self.stride,\n",
    "            groups=self.groups)\n",
    "\n",
    "        # the second conv\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.do2 = nn.Dropout(p=0.5)\n",
    "        self.conv2 = MyConv1dPadSame(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            groups=self.groups)\n",
    "\n",
    "        self.max_pool = MyMaxPool1dPadSame(kernel_size=self.stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = x\n",
    "\n",
    "        # the first conv\n",
    "        out = x\n",
    "        if not self.is_first_block:\n",
    "            if self.use_bn:\n",
    "                out = self.bn1(out)\n",
    "            out = self.relu1(out)\n",
    "            if self.use_do:\n",
    "                out = self.do1(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        # the second conv\n",
    "        if self.use_bn:\n",
    "            out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        if self.use_do:\n",
    "            out = self.do2(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # if downsample, also downsample identity\n",
    "        if self.downsample:\n",
    "            identity = self.max_pool(identity)\n",
    "\n",
    "        # if expand channel, also pad zeros to identity\n",
    "        if self.out_channels != self.in_channels:\n",
    "            identity = identity.transpose(-1,-2)\n",
    "            ch1 = (self.out_channels-self.in_channels)//2\n",
    "            ch2 = self.out_channels-self.in_channels-ch1\n",
    "            identity = F.pad(identity, (ch1, ch2), \"constant\", 0)\n",
    "            identity = identity.transpose(-1,-2)\n",
    "\n",
    "        # shortcut\n",
    "        out += identity\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "        X: (n_samples, n_channel, n_length)\n",
    "        Y: (n_samples)\n",
    "\n",
    "    Output:\n",
    "        out: (n_samples)\n",
    "\n",
    "    Pararmetes:\n",
    "        in_channels: dim of input, the same as n_channel\n",
    "        base_filters: number of filters in the first several Conv layer, it will double at every 4 layers\n",
    "        kernel_size: width of kernel\n",
    "        stride: stride of kernel moving\n",
    "        groups: set larget to 1 as ResNeXt\n",
    "        n_block: number of blocks\n",
    "        n_classes: number of classes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, base_filters, kernel_size, stride, groups, n_block, n_classes, downsample_gap=2, increasefilter_gap=4, use_bn=True, use_do=True, verbose=False):\n",
    "        super(ResNet1D, self).__init__()\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.n_block = n_block\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.use_bn = use_bn\n",
    "        self.use_do = use_do\n",
    "\n",
    "        self.downsample_gap = downsample_gap # 2 for base model\n",
    "        self.increasefilter_gap = increasefilter_gap # 4 for base model\n",
    "\n",
    "        # first block\n",
    "        self.first_block_conv = MyConv1dPadSame(in_channels=in_channels, out_channels=base_filters, kernel_size=self.kernel_size, stride=1)\n",
    "        self.first_block_bn = nn.BatchNorm1d(base_filters)\n",
    "        self.first_block_relu = nn.ReLU()\n",
    "        out_channels = base_filters\n",
    "\n",
    "        # residual blocks\n",
    "        self.basicblock_list = nn.ModuleList()\n",
    "        for i_block in range(self.n_block):\n",
    "            # is_first_block\n",
    "            if i_block == 0:\n",
    "                is_first_block = True\n",
    "            else:\n",
    "                is_first_block = False\n",
    "            # downsample at every self.downsample_gap blocks\n",
    "            if i_block % self.downsample_gap == 1:\n",
    "                downsample = True\n",
    "            else:\n",
    "                downsample = False\n",
    "            # in_channels and out_channels\n",
    "            if is_first_block:\n",
    "                in_channels = base_filters\n",
    "                out_channels = in_channels\n",
    "            else:\n",
    "                # increase filters at every self.increasefilter_gap blocks\n",
    "                in_channels = int(base_filters*2**((i_block-1)//self.increasefilter_gap))\n",
    "                if (i_block % self.increasefilter_gap == 0) and (i_block != 0):\n",
    "                    out_channels = in_channels * 2\n",
    "                else:\n",
    "                    out_channels = in_channels\n",
    "\n",
    "            tmp_block = BasicBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                stride = self.stride,\n",
    "                groups = self.groups,\n",
    "                downsample=downsample,\n",
    "                use_bn = self.use_bn,\n",
    "                use_do = self.use_do,\n",
    "                is_first_block=is_first_block)\n",
    "            self.basicblock_list.append(tmp_block)\n",
    "\n",
    "        # final prediction\n",
    "        self.final_bn = nn.BatchNorm1d(out_channels)\n",
    "        self.final_relu = nn.ReLU(inplace=True)\n",
    "        # self.do = nn.Dropout(p=0.5)\n",
    "        self.dense = nn.Linear(out_channels, n_classes)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x\n",
    "\n",
    "        # first conv\n",
    "        if self.verbose:\n",
    "            print('input shape', out.shape)\n",
    "        out = self.first_block_conv(out)\n",
    "        if self.verbose:\n",
    "            print('after first conv', out.shape)\n",
    "        if self.use_bn:\n",
    "            out = self.first_block_bn(out)\n",
    "        out = self.first_block_relu(out)\n",
    "\n",
    "        # residual blocks, every block has two conv\n",
    "        for i_block in range(self.n_block):\n",
    "            net = self.basicblock_list[i_block]\n",
    "            if self.verbose:\n",
    "                print('i_block: {0}, in_channels: {1}, out_channels: {2}, downsample: {3}'.format(i_block, net.in_channels, net.out_channels, net.downsample))\n",
    "            out = net(out)\n",
    "            if self.verbose:\n",
    "                print(out.shape)\n",
    "\n",
    "        # final prediction\n",
    "        if self.use_bn:\n",
    "            out = self.final_bn(out)\n",
    "        out = self.final_relu(out)\n",
    "        out = out.mean(-1)\n",
    "        if self.verbose:\n",
    "            print('final pooling', out.shape)\n",
    "        # out = self.do(out)\n",
    "        out = self.dense(out)\n",
    "        if self.verbose:\n",
    "            print('dense', out.shape)\n",
    "        # out = self.softmax(out)\n",
    "        if self.verbose:\n",
    "            print('softmax', out.shape)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def solute_3_classes(solute, df):\n",
    "    row = df.loc[df['SoluteName'] == solute].iloc[0]\n",
    "    out = torch.tensor((row.Level1, row.Level2, row.Level3))\n",
    "    return out\n",
    "\n",
    "\n",
    "# solute_3_classes('hydrogen', df3)\n",
    "def solute_TESA(solute, df):\n",
    "    row = df[df['SoluteName'] == solute][:1]\n",
    "    out = row[row.columns[17:26]]\n",
    "    out = torch.tensor(out.values, dtype=torch.float)\n",
    "    # print(f'type TESA: {out.dtype}')\n",
    "    return out\n",
    "\n",
    "\n",
    "# solute_TESA('hydrogen', df3)\n",
    "\n",
    "def solvent_macro_props1(solvent, path_to_table):\n",
    "    table = pd.read_table(path_to_table)\n",
    "    row = table[table['Name'] == solvent]\n",
    "    out = row[row.columns[2:]]\n",
    "    out = torch.tensor(out.values, dtype=torch.float)\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "octanol-water\n",
      "diethylether-water\n",
      "chloroform-water\n",
      "heptane-water\n",
      "isopropyltoluene\n",
      "cyclohexane-water\n",
      "benzene-water\n",
      "ethylacetate-water\n",
      "dichloroethane-water\n",
      "carbontet-water\n",
      "hexane-water\n",
      "butanol-water\n",
      "dibutylether-water\n",
      "chlorobenzene-water\n",
      "dibromoethane-water\n",
      "nitrobenzene-water\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Deleting charges species\n",
    "filename = r'/Users/balepka/Yandex.Disk.localized/Study/Lab/Neural Network/MNSol-v2009_energies_v2.tsv'\n",
    "with open(filename) as f:\n",
    "    t = 0\n",
    "    data = pd.read_table(f)\n",
    "    df1 = pd.DataFrame(data)\n",
    "\n",
    "df2 = df1.loc[df1['Charge'].isin([0])]\n",
    "\n",
    "# Deleting solvent mixtures\n",
    "filename2 = r'/Users/balepka/Yandex.Disk.localized/Study/Lab/Neural Network/MNSolDatabase_v2012/Solvent_properties.tsv'\n",
    "with open(filename2) as f:\n",
    "    data = pd.read_table(f, header=1)\n",
    "    solvent_props = pd.DataFrame(data)\n",
    "names = []\n",
    "for name, count in df2['Solvent'].value_counts().items():\n",
    "    row = solvent_props.loc[solvent_props['Name'] == name]\n",
    "    values = np.array(row[['nD', 'alpha', 'beta', 'gamma', 'epsilon', 'phi', 'psi']])\n",
    "    # print(f'{name} -> {count} -> {values.shape[0]}')\n",
    "    if values.shape[0] == 0:\n",
    "        print(name)\n",
    "        names.append(name)\n",
    "df3 = df2.loc[~df2['Solvent'].isin(names)]\n",
    "df3 = df3[df3.SoluteName != 'water dimer']\n",
    "\n",
    "## Creating table solvent - solute\n",
    "Solvents = dict(df3['Solvent'].value_counts().items())\n",
    "\n",
    "Solutes = dict(df3['SoluteName'].value_counts().items())\n",
    "from collections import OrderedDict\n",
    "\n",
    "Solvents = OrderedDict(df3['Solvent'].value_counts().items())\n",
    "Solutes = OrderedDict(df3['SoluteName'].value_counts().items())\n",
    "table_SS = pd.DataFrame(index=Solutes, columns=Solvents)\n",
    "\n",
    "# testa = df3.loc[df3['Solvent'] == 'water'].loc[df3['SoluteName'] == 'hydrogen']['DeltaGsolv']\n",
    "# testa\n",
    "# if testa.empty:\n",
    "#     print(1)\n",
    "# else:\n",
    "#     print(0)\n",
    "\n",
    "for solute in Solutes:\n",
    "    for solvent in Solvents:\n",
    "        SS_row = df3.loc[df3['Solvent'] == solvent].loc[df3['SoluteName'] == solute]['DeltaGsolv']\n",
    "        if SS_row.empty:\n",
    "            pass\n",
    "        else:\n",
    "            table_SS[solvent][solute] = SS_row.item()\n",
    "\n",
    "table_SS.to_csv('/Users/balepka/Yandex.Disk.localized/Study/Lab/Neural Network/Dataset preparation/table_SS.csv')\n",
    "\n",
    "# table_SS.head()\n",
    "table_SS.columns.tolist()\n",
    "table_SS.index.tolist()\n",
    "for solvent in table_SS.columns.tolist():\n",
    "    for solute in table_SS.index.tolist():\n",
    "        G_solv = table_SS[solvent][solute]\n",
    "        if not pd.isna(G_solv):\n",
    "            verboseprint(f's:{solvent} - {solute}: {G_solv}')\n",
    "        else:\n",
    "            verboseprint(f'NaN:{solvent} - {solute}: {G_solv}')\n",
    "\n",
    "\n",
    "def test_sp(solvent):\n",
    "    return torch.tensor(len(solvent))\n",
    "\n",
    "\n",
    "def test_up(solute):\n",
    "    return torch.tensor(len(solute))\n",
    "\n",
    "\n",
    "## Creating SS Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SS_Dataset(Dataset):\n",
    "    def __init__(self, table, solvent_props, solute_props, args = ((), ()), transform=None):\n",
    "        self.table = table\n",
    "        self.solvent_props = solvent_props\n",
    "        self.solute_props = solute_props\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "        self.table = self.table.set_index('Unnamed: 0')\n",
    "        self.sp, self.up = args\n",
    "        for solvent in self.table.columns.tolist():\n",
    "            for solute in self.table.index.tolist():\n",
    "                G_solv = self.table[solvent][solute]\n",
    "                if not pd.isna(G_solv):\n",
    "                    self.data.append((solvent, solute, G_solv))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        solvent, solute, G_solv = self.data[i]\n",
    "        X1 = self.solvent_props(solvent, self.sp)\n",
    "        X2 = self.solute_props(solute, self.up)\n",
    "        #check dim\n",
    "        # print(f'X1 - {X1.shape}')\n",
    "        # print(f'X2 - {X2.shape}')\n",
    "        verboseprint(f'X1 {X1.dtype}]')\n",
    "        verboseprint(f'X2 {X2.dtype}]')\n",
    "        X = torch.cat((X1, X2), 1)\n",
    "        y = torch.tensor(G_solv)\n",
    "        return X.float(), y.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "table_v1 = pd.read_csv(\n",
    "    '/Users/balepka/Yandex.Disk.localized/Study/Lab/Neural Network/Dataset preparation/table_SS_v1/table_SS_v1.csv')\n",
    "# table_v1.head()\n",
    "\n",
    "# table_v1.shape\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "table_v1 = pd.read_csv(\n",
    "    '/Users/balepka/Yandex.Disk.localized/Study/Lab/Neural Network/Dataset preparation/table_SS_v1/table_SS_v1.csv')\n",
    "path_props = '/Users/balepka/Yandex.Disk.localized/Study/Lab/Neural Network/MNSolDatabase_v2012/Solvent_properties1.tsv'\n",
    "TESA_df = df3\n",
    "args = (path_props, TESA_df)\n",
    "dataset = SS_Dataset(table_v1, solvent_macro_props1, solute_TESA, args)\n",
    "len_data = dataset.__len__()\n",
    "val_data = len_data // 10\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [len_data - val_data, val_data])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def validate(model, val_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_MSE = 0\n",
    "    loss = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for vector, G_true in val_loader:\n",
    "            vector, G_true = vector.to('cpu'), G_true.to('cpu')\n",
    "            model.to('cpu')\n",
    "            outputs = model(vector)\n",
    "            total += G_true.size(0)\n",
    "            all_MSE += loss(outputs.flatten(), G_true)\n",
    "            # print(f'pr: {predicted}')\n",
    "            # print(f'lab: {labels}')\n",
    "\n",
    "    return all_MSE / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def train(model, loss_function, optimizer,  epochs = 10, tag = \"cifar10\"):\n",
    "  for epoch in range(epochs):\n",
    "    hist_loss = 0\n",
    "    for vector, G_true in tqdm(train_loader): # get bacth\n",
    "        vector, G_true = vector.to(device), G_true.to(device)\n",
    "        model.to(device)\n",
    "        outputs = model(vector) # call forward inside\n",
    "\n",
    "        loss = loss_function(outputs, G_true) # calculate loss\n",
    "        loss.backward() # calculate gradients\n",
    "        optimizer.step() # performs a single optimization step (parameter update).\n",
    "        optimizer.zero_grad() # sets the gradients of all optimized tensors to zero.\n",
    "\n",
    "        hist_loss += loss.item() # For stat only\n",
    "\n",
    "    # writer.add_scalar(\"Train/Loss\", hist_loss / ( len(trainloader) ), epoch)\n",
    "    # train_loss_data[tag].append(hist_loss/(len(trainloader)))\n",
    "    # writer.close()\n",
    "    # # train_accuracy = get_accuracy(model, trainloader)\n",
    "    # writer.add_scalar(\"Train/Acc\", train_accuracy, epoch)\n",
    "    # # train_acc_data[tag].append(train_accuracy)\n",
    "    # writer.close()\n",
    "\n",
    "    # test_loss = 0\n",
    "    # for images, labels in tqdm(testloader): # get bacth\n",
    "    #     outputs = model(images) # call forward inside\n",
    "\n",
    "    #     loss = loss_function(outputs, labels) # calculate loss\n",
    "    #     test_loss += loss.item() # For stat only\n",
    "    # writer.add_scalar(\"Test/Loss\", test_loss / ( len(testloader) ), epoch)\n",
    "    # # test_loss_data[tag].append(test_loss/(len(testloader)))\n",
    "\n",
    "    # writer.close()\n",
    "\n",
    "    accuracy = validate(model, val_loader)\n",
    "    # writer.add_scalar(\"Test/Acc\", accuracy, epoch)\n",
    "    # test_acc_data[tag].append(accuracy)\n",
    "    # writer.close()\n",
    "\n",
    "    # accuracy = validate(model, testloader)\n",
    "    print(print(f'epoch {epoch} -> {accuracy}'))\n",
    "  return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 -> 0.2731708586215973\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 -> 0.15287929773330688\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 -> 0.15542353689670563\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 -> 0.15314002335071564\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 -> 0.15225361287593842\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 -> 0.1563526839017868\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 -> 0.15347057580947876\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 -> 0.1537737101316452\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 12.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 -> 0.1521931290626526\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 -> 0.15166333317756653\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(0.1517)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "in_channels = next(iter(train_loader))[0].shape[1]\n",
    "model = ResNet1D(in_channels, base_filters=2, kernel_size=3, stride=2, groups=1, n_block=3, n_classes=1, use_bn=True, use_do=True, verbose=False)\n",
    "model.to(device)  # Create model instance\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)  # Weight update\n",
    "# reinit_tensorboard(clear_log = True)\n",
    "my_acc =  train(model, nn.MSELoss(), optimizer, epochs = 10, tag = 'ResNET 1D' )\n",
    "my_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}